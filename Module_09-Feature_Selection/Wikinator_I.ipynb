{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandini040601/Kaggle-Datasets/blob/main/Module_09-Feature_Selection/Wikinator_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikinator I\n",
        "\n",
        "Name\n",
        "\n"
      ],
      "metadata": {
        "id": "vqp8RGHajdVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC5502 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "uRfFylCjbyMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Object Model (DOM)\n",
        "\n",
        "The Document Object Model (DOM) is a cross-platform and language-independent interface that treats an HTML or XML document as a tree structure wherein each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. Nodes can have event handlers (also known as event listeners) attached to them. Once an event is triggered, the event handlers get executed.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Document_Object_Model"
      ],
      "metadata": {
        "id": "M8tEXHx6ZKTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikipedia API\n",
        "\n",
        "If you intend to do any scraping projects or automated requests, consider alternatives such as Pywikipediabot or MediaWiki API, which has other superior features.\n",
        "\n",
        "* wikipedia.search('keywords', results=2)\n",
        "* wikipedia.suggest('keyword')\n",
        "* wikipedia.summary('keywords', sentences=2)\n",
        "* wikipedia.page('keywords')\n",
        "* wikipedia.page('keywords').content\n",
        "* wikipedia.page('keywords').references\n",
        "* wikipedia.page('keywords').title\n",
        "* wikipedia.page('keywords').url\n",
        "* wikipedia.page('keywords').categories\n",
        "* wikipedia.page('keywords').content\n",
        "* wikipedia.page('keywords').links\n",
        "* wikipedia.geosearch(33.2075, 97.1526)\n",
        "* wikipedia.set_lang('hi')\n",
        "* wikipedia.languages()\n",
        "* wikipedia.page('keywords').images[0]\n",
        "* wikipedia.page('keywords').html()"
      ],
      "metadata": {
        "id": "IkcUFya3UHcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beautiful Soup"
      ],
      "metadata": {
        "id": "MeHz5UrL7dpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install wikipedia"
      ],
      "metadata": {
        "id": "G_gD15toS2rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://kleiber.me/blog/2017/07/22/tutorial-lda-wikipedia/\n",
        "# import pandas as pd\n",
        "# import random\n",
        "# import wikipedia\n",
        "\n",
        "# # rtitles = wikipedia.random(5)\n",
        "\n",
        "# # get 5 Wikipedia page titles based on keywords\n",
        "# titles = []\n",
        "# keywords = ['ultranationalism', 'religion', 'religious facism', 'state religion', 'deifying rulers']\n",
        "# for key in keywords:\n",
        "#     title = wikipedia.search(key, results=5)\n",
        "#     titles.append(title[0])\n",
        "\n",
        "# data = []\n",
        "\n",
        "# for title in titles:\n",
        "#     # disambiguous error fix\n",
        "#     try:\n",
        "#         url_title = title.strip().replace(' ', '_')\n",
        "#         url = f'https://en.wikipedia.org/wiki/{url_title}' # left alt, shift, down to duplicate line\n",
        "#         # data.append([title, url, wikipedia.page(title, auto_suggest=False).content, wikipedia.summary(title, auto_suggest=False, sentences=15)])\n",
        "#         data.append([title, url])\n",
        "#     except wikipedia.exceptions.DisambiguationError as e:\n",
        "#         s = random.choice(e.options)\n",
        "#         data.append([title, wikipedia.page(s).content,  wikipedia.summary(title, auto_suggest=False, sentences=15)])\n",
        "\n",
        "# pages = pd.DataFrame(data, columns=['title', 'url'])\n",
        "# pages.head()"
      ],
      "metadata": {
        "id": "Cn93bMfERxsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# import pprint\n",
        "# import re\n",
        "\n",
        "# CLEANR = re.compile('<.*?>')\n",
        "# def cleanhtml(raw_html):\n",
        "#   cleantext = re.sub(CLEANR, '', raw_html)\n",
        "#   return cleantext\n",
        "\n",
        "# data = []\n",
        "# def get_subs(row):\n",
        "#   title = row['title']\n",
        "#   url = row['url']\n",
        "\n",
        "#   response = requests.get(url)\n",
        "#   html_content = response.text\n",
        "\n",
        "#   soup = BeautifulSoup(html_content, 'html.parser')\n",
        "#   h2_tags = soup.find_all('h2')\n",
        "\n",
        "#   # Get the text at the beginning of the article, before the first h2 tag\n",
        "#   content_div = soup.find('div', class_='mw-parser-output')\n",
        "#   text_before_first_h2 = \"\"\n",
        "#   for element in content_div.children:\n",
        "#       if element.name == 'p':\n",
        "#           text_before_first_h2 += element.get_text(strip=False) + \"\\n\"\n",
        "#       elif element.name == 'h2':\n",
        "#           break\n",
        "\n",
        "#   text_before_first_h2 = text_before_first_h2.replace(\"\\n\", \" \")  # Replace newlines with spaces\n",
        "#   text_before_first_h2 = re.sub(r\"\\[\\d+\\]\", \"\", text_before_first_h2)\n",
        "\n",
        "#   data.append([title, url, 'Intro', cleanhtml(text_before_first_h2)])\n",
        "\n",
        "#   # Extract text between h2 tags\n",
        "#   for i in range(len(h2_tags) - 1):\n",
        "#       h2_tag = h2_tags[i]\n",
        "#       next_h2_tag = h2_tags[i + 1]\n",
        "\n",
        "#       elements_between_h2s = h2_tag.find_all_next(string=True, limit=next_h2_tag.sourcepos)\n",
        "#       text_between_h2s = \"\"\n",
        "#       for element in elements_between_h2s:\n",
        "#           if element.parent.name == 'p':\n",
        "#               # Get the text content of the <p> tag, including the text within <a> tags\n",
        "#               for content in element.parent.contents:\n",
        "#                   if isinstance(content, str):\n",
        "#                       text_between_h2s += content\n",
        "#                   elif content.name == 'a':\n",
        "#                       text_between_h2s += content.text\n",
        "#               # text_between_h2s += \"\\n\"  # Add a newline after each <p>\n",
        "\n",
        "#       # print(f\"Text between '{h2_tag.text}' and '{next_h2_tag.text}':\\n{text_between_h2s}\\n\")\n",
        "#       data.append([title, url, h2_tag.text, cleanhtml(text_between_h2s)])\n",
        "\n",
        "#   # Handle the last h2 tag\n",
        "#   last_h2_tag = h2_tags[-1]\n",
        "#   elements_after_last_h2 = last_h2_tag.find_all_next(string=True)\n",
        "#   text_after_last_h2 = \"\"\n",
        "#   for element in elements_after_last_h2:\n",
        "#       if element.parent.name == 'p':\n",
        "#           # Get the text content of the <p> tag, including the text within <a> tags\n",
        "#           for content in element.parent.contents:\n",
        "#               if isinstance(content, str):\n",
        "#                   text_after_last_h2 += content\n",
        "#               elif content.name == 'a':\n",
        "#                   text_after_last_h2 += content.text\n",
        "#           text_after_last_h2 += \"\\n\"  # Add a newline after each <p>\n",
        "\n",
        "#   # print(f\"Text after '{last_h2_tag.text}':\\n{text_after_last_h2}\\n\")\n",
        "#   data.append([title, url, last_h2_tag.text, cleanhtml(text_after_last_h2)])\n",
        "\n",
        "# x = pages.apply(get_subs, axis=1)\n",
        "# # df = pd.DataFrame(data, columns=['title', 'url', 'heading', 'subheading', 'txt'])\n",
        "# df = pd.DataFrame(data, columns=['title', 'url', 'heading', 'txt'])\n",
        "# print(df.shape)\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "KN48B3qL3Sc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete rows with Contents and no txt\n"
      ],
      "metadata": {
        "id": "RfVK8QS_Edpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the txt of the first row"
      ],
      "metadata": {
        "id": "SuXsShneGKKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# title value counts"
      ],
      "metadata": {
        "id": "Gr35ooXCgvls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA (Latent Dirichlet Allocation)\n",
        "\n",
        "In natural language processing, latent Dirichlet allocation (LDA) is a Bayesian network (and, therefore, a generative statistical model) for modeling automatically extracted topics in textual corpora. The LDA is an example of a Bayesian topic model. In this, observations (e.g., words) are collected into documents, and each word's presence is attributable to one of the document's topics. Each document will contain a small number of topics.\n",
        "\n",
        "Sources:\n",
        " * https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
        " * https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
        "\n",
        "Natural Language Processing Basics\n",
        "\n",
        "* https://github.com/gitmystuff/DSChunks/blob/main/Natural%20Language%20Processing.ipynb"
      ],
      "metadata": {
        "id": "zveASyBdXR5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More on LDA\n",
        "\n",
        "Gemini, October 17 2024\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a statistical method used in natural language processing to uncover hidden thematic structures within a collection of documents. In simpler terms, it's a way to discover the main topics that a set of documents is talking about.\n",
        "\n",
        "Here's a breakdown of the key concepts:\n",
        "\n",
        "**1. Documents as Mixtures of Topics**\n",
        "\n",
        "LDA assumes that each document is a mixture of various topics. For example, a news article might be 70% about politics, 20% about economics, and 10% about sports.\n",
        "\n",
        "**2. Topics as Distributions of Words**\n",
        "\n",
        "Each topic is represented as a probability distribution over words. Words that are highly probable within a topic are the words that best characterize that topic. For instance, a \"politics\" topic might have high probabilities for words like \"election,\" \"government,\" \"president,\" etc.\n",
        "\n",
        "**3. The Generative Process**\n",
        "\n",
        "LDA imagines a process for generating documents:\n",
        "\n",
        "   * For each document:\n",
        "      * Choose a distribution of topics (e.g., 70% politics, 20% economics, 10% sports).\n",
        "      * For each word in the document:\n",
        "         * Choose a topic from the document's distribution of topics.\n",
        "         * Choose a word from the chosen topic's distribution of words.\n",
        "\n",
        "**4. Learning the Topics**\n",
        "\n",
        "LDA's main task is to learn these topic distributions from a given set of documents. It uses statistical inference to figure out which words belong to which topics and how much each topic contributes to each document.\n",
        "\n",
        "**5. Dirichlet Distributions**\n",
        "\n",
        "LDA uses Dirichlet distributions as priors for the topic distributions. These priors act as a kind of \"regularization\" that helps prevent overfitting and encourages the model to find meaningful topics.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "LDA is like a powerful \"unmixing\" tool. It takes a collection of documents, which are like mixed signals, and separates them into their underlying thematic components. This allows us to understand the main themes present in the data and how those themes are distributed across the documents.\n",
        "\n",
        "**Here's an analogy:**\n",
        "\n",
        "Imagine you have a bowl of mixed candies. Each candy represents a word, and each handful of candies represents a document. LDA is like sorting those candies into separate piles based on their type (e.g., chocolate, gummy, sour). Each pile represents a topic, and the proportion of each type of candy in a handful tells you the mixture of topics in that document.\n",
        "\n",
        "**Applications of LDA:**\n",
        "\n",
        "* **Topic modeling:** Discovering the main topics in a corpus of text data (e.g., news articles, social media posts, scientific papers).\n",
        "* **Document classification:** Assigning documents to predefined categories based on their topic distributions.\n",
        "* **Content recommendation:** Recommending documents or articles to users based on their interests, which can be inferred from their reading history.\n",
        "* **Customer segmentation:** Grouping customers based on their feedback or reviews, allowing businesses to tailor their marketing strategies.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g4AwRiFrArUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More on TfidfVectorizer\n",
        "\n",
        "Gemini, October 17 2024\n",
        "\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "\n",
        "TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It's calculated as the product of two metrics:\n",
        "\n",
        "* **Term Frequency (TF):**  This measures how frequently a term appears in a document. A higher TF indicates that the term is more important within that specific document.\n",
        "\n",
        "* **Inverse Document Frequency (IDF):** This measures how rare a term is across the entire corpus.  Words that appear in many documents have a low IDF, while words that appear in only a few documents have a high IDF. This helps to downweight common words like \"the,\" \"a,\" and \"is,\" which are less informative for understanding the document's content.\n",
        "\n",
        "**The TF-IDF formula:**\n",
        "\n",
        "```\n",
        "TF-IDF(term, document) = TF(term, document) * IDF(term)\n",
        "```\n",
        "\n",
        "**Why is TF-IDF useful?**\n",
        "\n",
        "TF-IDF helps to identify the most distinctive words in each document, effectively highlighting the words that are most relevant to that document's specific topic or meaning. This is crucial for tasks like:\n",
        "\n",
        "* **Information retrieval:** Ranking documents based on their relevance to a search query.\n",
        "* **Text mining:** Identifying key themes and concepts in a collection of documents.\n",
        "* **Document classification:** Categorizing documents based on their content.\n",
        "\n",
        "**TfidfVectorizer**\n",
        "\n",
        "TfidfVectorizer is a class in the scikit-learn library (a popular Python machine learning library) that automates the process of calculating TF-IDF features from text data. It takes raw text documents as input and converts them into a matrix of TF-IDF features.\n",
        "\n",
        "**Here's what TfidfVectorizer does:**\n",
        "\n",
        "1. **Tokenization:**  It breaks down the text into individual words or tokens.\n",
        "2. **Count Vectorization:** It counts the occurrences of each word in each document, creating a \"bag-of-words\" representation.\n",
        "3. **TF-IDF Transformation:** It calculates the TF-IDF score for each word in each document, generating a matrix where each row represents a document, and each column represents a word.\n",
        "\n",
        "**Example using scikit-learn:**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(tfidf_matrix)\n",
        "```\n",
        "\n",
        "**Key advantages of using TfidfVectorizer:**\n",
        "\n",
        "* **Efficiency:** It handles the entire TF-IDF calculation process, saving you time and effort.\n",
        "* **Customization:** It offers various parameters to fine-tune the process, such as controlling the minimum and maximum document frequency of words to be included, removing stop words, and applying different tokenization methods.\n",
        "* **Integration:** It seamlessly integrates with other scikit-learn tools and pipelines for building machine learning models.\n",
        "\n",
        "In essence, TF-IDF is a powerful technique for quantifying the importance of words in documents, and TfidfVectorizer provides a convenient way to apply this technique in practice.\n"
      ],
      "metadata": {
        "id": "j9OJs4OzBMOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.decomposition import LatentDirichletAllocation\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# results = 10\n",
        "# components = 10\n",
        "# topics = 10\n",
        "\n",
        "# vectorizer = TfidfVectorizer(stop_words='english')\n",
        "# vectors = vectorizer.fit_transform(df['txt'].values.astype('U'))\n",
        "\n",
        "# model = LatentDirichletAllocation(n_components=components)\n",
        "# model.fit(vectors)\n",
        "\n",
        "# topics_dictionary = {}\n",
        "# for index, topic in enumerate(model.components_):\n",
        "#     print(f'Topic {index} top words: {[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-topics:]]}')\n",
        "#     topics_dictionary[index] = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-topics:]]"
      ],
      "metadata": {
        "id": "OE2FwJ7dXA0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get topics"
      ],
      "metadata": {
        "id": "5JXDOSqGpNb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SpaCy\n",
        "\n",
        "* https://spacy.io/\n",
        "* https://medium.com/analytics-vidhya/text-summarization-using-spacy-ca4867c6b744"
      ],
      "metadata": {
        "id": "0X9RwanIZOh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "xJMehakYagk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Model and Pipelines\n",
        "\n",
        "en_core_web_sm\n",
        "\n",
        "* https://www.kdnuggets.com/2021/03/natural-language-processing-pipelines-explained.html\n",
        "* https://spacy.io/usage/spacy-101\n",
        "* https://en.wikipedia.org/wiki/Language_model\n",
        "* https://builtin.com/data-science/beginners-guide-language-models"
      ],
      "metadata": {
        "id": "ma6EDIYYnp-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More on SpaCy and en_core_web_sm\n",
        "\n",
        "Gemini, October 17 2024\n",
        "\n",
        "**spaCy: The NLP Library**\n",
        "\n",
        "Think of spaCy as a powerful toolbox for Natural Language Processing (NLP) tasks. It provides a wide range of functionalities, including:\n",
        "\n",
        "* **Tokenization:** Breaking down text into individual words or tokens.\n",
        "* **Part-of-speech (POS) tagging:** Identifying the grammatical role of each word (e.g., noun, verb, adjective).\n",
        "* **Named Entity Recognition (NER):** Identifying and classifying named entities like people, organizations, and locations.\n",
        "* **Dependency parsing:** Analyzing the grammatical structure of sentences.\n",
        "* **Lemmatization:** Reducing words to their base form (e.g., \"running\" to \"run\").\n",
        "\n",
        "**`en_core_web_sm`: The Language Model**\n",
        "\n",
        "Now, `en_core_web_sm` is a specific **trained language model** for English that you use *with* spaCy. It's like a pre-built engine that powers spaCy's NLP capabilities.\n",
        "\n",
        "Here's what `en_core_web_sm` provides:\n",
        "\n",
        "* **Statistical models:** These models have been trained on a large amount of English text data, enabling spaCy to make accurate predictions about things like POS tags, named entities, and syntactic dependencies.\n",
        "* **Vocabulary:** A comprehensive vocabulary of English words with their associated linguistic properties.\n",
        "* **Language-specific rules:** Rules and exceptions that are specific to the English language.\n",
        "\n",
        "**The Connection**\n",
        "\n",
        "To use spaCy for NLP tasks on English text, you need to load a language model like `en_core_web_sm`. This model provides the necessary data and algorithms for spaCy to process and understand the text.\n",
        "\n",
        "**Here's how it works in code:**\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process a text\n",
        "text = \"This is an example sentence.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Access linguistic annotations\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "* `spacy.load(\"en_core_web_sm\")` loads the `en_core_web_sm` model.\n",
        "* `nlp(text)` processes the text using the loaded model.\n",
        "* The code then accesses linguistic annotations like POS tags (`token.pos_`) and dependency labels (`token.dep_`) provided by the model.\n",
        "\n",
        "**Different Model Sizes**\n",
        "\n",
        "spaCy offers different sizes of English language models:\n",
        "\n",
        "* `en_core_web_sm` (small):  A smaller model that's faster but may be less accurate.\n",
        "* `en_core_web_md` (medium): A balance of speed and accuracy.\n",
        "* `en_core_web_lg` (large): A larger model that's more accurate but slower.\n",
        "\n",
        "You choose the model that best suits your needs based on factors like speed, accuracy requirements, and computational resources.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "spaCy is the framework, and `en_core_web_sm` is the engine that makes it work for English text. You need both to perform NLP tasks effectively.\n"
      ],
      "metadata": {
        "id": "th7GkChLCMAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# from spacy.lang.en.stop_words import STOP_WORDS\n",
        "# from string import punctuation\n",
        "# from collections import Counter\n",
        "# from heapq import nlargest\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "xAuRN6jpYhrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get example text\n"
      ],
      "metadata": {
        "id": "9HnKgee9hjLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of Speech Tagging\n",
        "\n",
        "Gemini, October 17 2024\n",
        "\n",
        "In Natural Language Processing (NLP), **POS stands for Part-of-Speech tagging**. It's the process of assigning a grammatical category or label to each word in a given text. These labels indicate the syntactic role that a word plays in a sentence.\n",
        "\n",
        "Think of it like this: when you were learning grammar in school, you probably learned about nouns, verbs, adjectives, adverbs, and so on. POS tagging does this automatically for computers.\n",
        "\n",
        "**Why is POS tagging important in NLP?**\n",
        "\n",
        "* **Understanding sentence structure:** POS tags help computers understand the grammatical relationships between words in a sentence, which is crucial for tasks like parsing and understanding the meaning of the text.\n",
        "* **Word sense disambiguation:**  Some words can have multiple meanings (e.g., \"bank\" can be a financial institution or the side of a river). POS tags can help determine the correct meaning based on the word's role in the sentence.\n",
        "* **Feature engineering for machine learning:** POS tags can be used as features in machine learning models for various NLP tasks, such as sentiment analysis, text classification, and named entity recognition.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider the sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "Here's how POS tagging would label the words:\n",
        "\n",
        "* The: determiner (DET)\n",
        "* quick: adjective (ADJ)\n",
        "* brown: adjective (ADJ)\n",
        "* fox: noun (NOUN)\n",
        "* jumps: verb (VERB)\n",
        "* over: preposition (ADP)\n",
        "* the: determiner (DET)\n",
        "* lazy: adjective (ADJ)\n",
        "* dog: noun (NOUN)\n",
        "\n",
        "**How does POS tagging work?**\n",
        "\n",
        "There are different approaches to POS tagging:\n",
        "\n",
        "* **Rule-based:** Uses handcrafted linguistic rules to assign tags.\n",
        "* **Statistical:** Employs statistical models trained on large annotated corpora.\n",
        "* **Machine learning:** Uses machine learning algorithms (like Hidden Markov Models or Conditional Random Fields) to learn patterns from data.\n",
        "\n",
        "**Tools for POS tagging:**\n",
        "\n",
        "Many NLP libraries provide tools for POS tagging:\n",
        "\n",
        "* **NLTK (Natural Language Toolkit):** A popular Python library with various POS taggers.\n",
        "* **spaCy:** Another powerful NLP library with fast and accurate POS tagging capabilities.\n",
        "* **Stanford CoreNLP:** A Java library with a wide range of NLP tools, including POS tagging.\n",
        "\n",
        "By understanding the grammatical structure of text, POS tagging helps computers make sense of human language and perform various NLP tasks more effectively.\n"
      ],
      "metadata": {
        "id": "QB-L85QHG0Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import textwrap\n",
        "# import re\n",
        "\n",
        "# summary_text = ' '.join([re.sub(\"\\[.*?\\]\", \"\", df.iloc[0]['txt'])])\n",
        "# doc = nlp(summary_text)\n",
        "# keyword = []\n",
        "# stopwords = list(STOP_WORDS)\n",
        "# pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
        "# for token in doc:\n",
        "#     if(token.text in stopwords or token.text in punctuation):\n",
        "#         continue\n",
        "#     if(token.pos_ in pos_tag):\n",
        "#         keyword.append(token.text)\n",
        "\n",
        "# freq_word = Counter(keyword)\n",
        "# max_freq = Counter(keyword).most_common(1)[0][1]\n",
        "# for word in freq_word.keys():\n",
        "#     freq_word[word] = (freq_word[word]/max_freq)\n",
        "\n",
        "# sent_strength={}\n",
        "# for sent in doc.sents:\n",
        "#     for word in sent:\n",
        "#         if word.text in freq_word.keys():\n",
        "#             if sent in sent_strength.keys():\n",
        "#                 sent_strength[sent] += freq_word[word.text]\n",
        "#             else:\n",
        "#                 sent_strength[sent] = freq_word[word.text]\n",
        "\n",
        "#     try:\n",
        "#       data.append([sent_strength[sent], str(sent)])\n",
        "#     except:\n",
        "#       pass\n",
        "#     print(sent_strength[sent])\n",
        "#     print(textwrap.fill(str(sent)))\n",
        "#     print()\n",
        "\n",
        "# summary = nlargest(10, sent_strength, key=sent_strength.get)\n",
        "# summary = ' '.join([w.text for w in summary])\n",
        "# print(textwrap.fill(summary, 100))\n",
        "# # df2 = pd.DataFrame(data, columns=['strength', 'txt'])\n",
        "# # df2.sort_values(by=['strength'], ascending=False).head()"
      ],
      "metadata": {
        "id": "gJuB77HNawNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet is designed to remove text within square brackets from a string variable called `summary`. Let's break down how it works:\n",
        "\n",
        "**1. `re.sub(\"\\[.*?\\]\", \"\", summary)`:**\n",
        "\n",
        "*   **`re.sub()`:** This is a function from Python's `re` module (regular expression module) used for string substitution. It finds all occurrences of a pattern in a string and replaces them with something else.\n",
        "*   **`\\[.*?\\]`:** This is the regular expression pattern. Let's decode it:\n",
        "    *   `\\[` and `\\]`: These match the literal square brackets \"[\" and \"]\".\n",
        "    *   `.`: This matches any character (except a newline).\n",
        "    *   `*`: This matches zero or more occurrences of the preceding character (in this case, any character).\n",
        "    *   `?`: This makes the `*` \"non-greedy,\" meaning it matches the shortest possible string that satisfies the pattern. This is important to avoid matching across multiple sets of brackets.\n",
        "*   **`\"\"`:** This is the replacement string. In this case, it's an empty string, meaning the matched pattern will be removed.\n",
        "*   **`summary`:** This is the input string where the substitution will be performed.\n",
        "\n",
        "This part effectively finds any text enclosed within square brackets (including the brackets themselves) and removes it from the `summary` string.\n",
        "\n",
        "**2. `' '.join([...])`:**\n",
        "\n",
        "*   **`[...]`:** The result of `re.sub()` is a string. This string is put inside a list. So, you have a list with a single string element.\n",
        "*   **`' '.join(...)`:** This joins the elements of the list (in this case, just the one string) using a space as the separator. This step might seem redundant since there's only one string in the list, but it's likely there because the code might have been intended to handle a list of strings at some point.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If `summary` was initially:\n",
        "\n",
        "```python\n",
        "summary = \"This is a sentence with [some text] in brackets.\"\n",
        "```\n",
        "\n",
        "After running the code snippet, `summary` would become:\n",
        "\n",
        "```\n",
        "\"This is a sentence with  in brackets.\"\n",
        "```\n",
        "\n",
        "**In essence, this code snippet removes any text enclosed in square brackets from a string.** It's often used to clean up text data by removing annotations, citations, or other unwanted elements that are consistently enclosed in brackets.\n"
      ],
      "metadata": {
        "id": "lc-qZDu2HQoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary recap"
      ],
      "metadata": {
        "id": "budLb2NnjpV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# from spacy.lang.en.stop_words import STOP_WORDS\n",
        "# from string import punctuation\n",
        "\n",
        "# nlp = spacy.blank('en')\n",
        "# nlp.add_pipe('sentencizer')\n",
        "\n",
        "# # https://www.educative.io/answers/text-summarization-in-spacy-and-nltk\n",
        "# # df.iloc[0]['txt']\n",
        "# def summarizer(row):\n",
        "#   txt = row['txt']\n",
        "#   text = ' '.join([re.sub('\\[.*?\\]|\"', '', txt)])\n",
        "#   doc = nlp(text)\n",
        "\n",
        "#   word_frequencies = {}\n",
        "#   for token in doc:\n",
        "#       if token.text not in STOP_WORDS and token.text not in punctuation:\n",
        "#           if token.text not in word_frequencies:\n",
        "#               word_frequencies[token.text] = 1\n",
        "#           else:\n",
        "#               word_frequencies[token.text] += 1\n",
        "\n",
        "\n",
        "#   sorted_sentences = sorted(doc.sents, key=lambda sent: sum(word_frequencies[token.text]\n",
        "#                           for token in sent if token.text in word_frequencies), reverse=True)\n",
        "\n",
        "#   # return str(' '.join(sent.text for sent in sorted_sentences[:int(len(sorted_sentences)/4)]).strip())\n",
        "#   return str(' '.join(sent.text for sent in sorted_sentences[:2]).strip())\n",
        "\n",
        "# # print(textwrap.fill(summarizer(df.iloc[0]['txt'])))"
      ],
      "metadata": {
        "id": "TAWScnDVm9Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create summary variable\n"
      ],
      "metadata": {
        "id": "-rbAKFljnEzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the iloc 4 row"
      ],
      "metadata": {
        "id": "qZY_i0fo6bmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print using summarizer"
      ],
      "metadata": {
        "id": "TFbj9p5q6D0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Gemini, October 17 2024\n",
        "\n",
        "**1. Choose Your Libraries**\n",
        "\n",
        "* **Extractive Summarization:**\n",
        "    * `sumy`: A popular library with various extractive methods (TextRank, LexRank, Luhn).\n",
        "    * `nltk`:  Provides tools for sentence tokenization and other NLP tasks.\n",
        "\n",
        "* **Generative Summarization:**\n",
        "    * `transformers`: Hugging Face's library for accessing pre-trained Transformer models (like BART, T5) for text generation.\n",
        "\n",
        "**2.  Prepare Your Data**\n",
        "\n",
        "* Choose a dataset or some sample articles for summarization.\n",
        "* Preprocess the text (clean up, tokenize, etc.).\n",
        "\n",
        "**3. Implement Extractive Summarization**\n",
        "\n",
        "```python\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer  # Example: Luhn method\n",
        "\n",
        "# Example using sumy\n",
        "def extractive_summary(text, num_sentences=3):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LuhnSummarizer()\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    return \" \".join([str(sentence) for sentence in summary])\n",
        "\n",
        "# ... (your data loading and preprocessing)\n",
        "\n",
        "extractive_result = extractive_summary(your_text_data)\n",
        "print(\"Extractive Summary:\", extractive_result)\n",
        "```\n",
        "\n",
        "**4. Implement Generative Summarization**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Example using transformers\n",
        "def generative_summary(text):\n",
        "    summarizer = pipeline('summarization', model=\"facebook/bart-large-cnn\")  # Example: BART model\n",
        "    summary_text = summarizer(text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
        "    return summary_text\n",
        "\n",
        "# ... (your data loading and preprocessing)\n",
        "\n",
        "generative_result = generative_summary(your_text_data)\n",
        "print(\"Generative Summary:\", generative_result)\n",
        "```\n",
        "\n",
        "**5. Compare the Results**\n",
        "\n",
        "* **Qualitative analysis:**  Examine the summaries generated by each method. How do they differ in terms of fluency, coherence, and factual accuracy?\n",
        "* **Quantitative metrics:** Use ROUGE scores (Recall-Oriented Understudy for Gisting Evaluation) to measure the overlap between the generated summaries and a reference summary (if available).\n",
        "* **Evaluation based on your use case:** Consider the specific requirements of your application. Does one method perform better in terms of preserving key information, conciseness, or readability?\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "* **Model Selection:** Experiment with different extractive methods (TextRank, LexRank) and generative models (BART, T5, Pegasus) to see which performs best for your data.\n",
        "* **Parameter Tuning:** Adjust parameters like the number of sentences for extractive methods and `max_length` for generative methods to optimize the summaries.\n",
        "* **Computational Resources:** Generative models generally require more computational power than extractive methods.\n",
        "\n",
        "This framework will help you compare extractive and generative summarization techniques in Python. Remember to analyze the results in the context of your specific needs and data.\n",
        "\n",
        "**Extractive Summarization**\n",
        "\n",
        "* **What it does:**  Selects the most important sentences from the original text and combines them to create a summary. Think of it like highlighting the key sentences in an article.\n",
        "* **How it works:**  Uses algorithms to rank sentences based on features like keyword frequency, position in the text, and similarity to the title.\n",
        "* **Pros:**\n",
        "    *  Fast and efficient.\n",
        "    *  Preserves the original wording, ensuring factual accuracy.\n",
        "* **Cons:**\n",
        "    * Can be disjointed and lack coherence since sentences are taken out of context.\n",
        "    * May not capture the overall meaning if the key information is not explicitly stated in a single sentence.\n",
        "\n",
        "**Generative Summarization**\n",
        "\n",
        "* **What it does:**  Generates a new summary that captures the essence of the original text in a concise and fluent way.  It's like paraphrasing and condensing the information.\n",
        "* **How it works:**  Uses deep learning models (like Transformers) to understand the meaning of the text and generate new sentences that convey the key information.\n",
        "* **Pros:**\n",
        "    * Can produce more concise and coherent summaries.\n",
        "    * Can rephrase information and potentially provide new insights.\n",
        "* **Cons:**\n",
        "    * More computationally expensive and slower.\n",
        "    * May sometimes generate inaccurate or irrelevant information.\n",
        "\n",
        "**Analogy**\n",
        "\n",
        "Imagine you're summarizing a long history chapter:\n",
        "\n",
        "* **Extractive:** Like copying and pasting the most important sentences from the chapter into a shorter document.\n",
        "* **Generative:** Like reading the chapter and then writing a new, shorter version in your own words that captures the main events and themes.\n",
        "\n",
        "**In a nutshell**\n",
        "\n",
        "Extractive summarization is like a highlighter, while generative summarization is like a writer.\n",
        "\n",
        "**Which is better?**\n",
        "\n",
        "It depends on your needs!\n",
        "\n",
        "* If speed and factual accuracy are paramount, extractive methods might be preferred.\n",
        "* If conciseness and fluency are more important, generative models are often better.\n",
        "\n",
        "The field of summarization is constantly evolving, with new and improved techniques being developed.\n"
      ],
      "metadata": {
        "id": "ZdYPTpgQDDHj"
      }
    }
  ]
}